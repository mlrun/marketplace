{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is to create a function to ingest data from snowflake with a Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dask frameworks enables users to parallelize their python code and run it as a distributed process on Iguazio cluster and dramatically accelerate their performance. <br>\n",
    "In this notebook we'll create an mlrun function running as a dask client to ingest data from snowflake. <br>\n",
    "It also demonstrates how to run parallelize query against snowflake using Dask Delayed option to query a large data set from snowflake. <br>\n",
    "The function will be published on the function marketplace. <br>\n",
    "For more information on dask over kubernetes: https://kubernetes.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "project_name = \"snowflake-dask\"\n",
    "dask_cluster_name=\"snowflake-dask-cluster\"\n",
    "artifact_path = mlrun.set_environment(project=project_name,\n",
    "                                      artifact_path = os.path.join(os.path.abspath('/v3io/projects/'), project_name))\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f'artifact_path = {artifact_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load snowflake configuration from config file. \n",
    "This is for demo purpose, in the real production code, you would need to put the snowflake connection info into secrets use the secrets in the running pod to connect to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load connection info\n",
    "with open(\".config.yaml\") as f:\n",
    "    connection_info = yaml.safe_load(f)\n",
    "\n",
    "# verify the config\n",
    "print(connection_info['account'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function querys data from snowflake using snowflake python connector for parallel processing of the query results. <br>\n",
    "With snoeflake python connector, when you execute a query, the cursor will return the result batches. <br>\n",
    "Using Dask Delayed it will return and process results set in parallel. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the function to a py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile snowflake_dask.py\n",
    "\"\"\"Snowflake Dask - Ingest Snowflake data with Dask\"\"\"\n",
    "import warnings\n",
    "import mlrun\n",
    "from mlrun.execution import MLClientCtx\n",
    "import snowflake.connector as snow\n",
    "from dask.distributed import Client\n",
    "from dask.dataframe import from_delayed\n",
    "from dask import delayed\n",
    "from dask import dataframe as dd\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@delayed\n",
    "def load(batch):\n",
    "\n",
    "    \"\"\"A delayed load one batch.\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"BATCHING\")\n",
    "        df_ = batch.to_pandas()\n",
    "        return df_\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {batch} for {e}\")\n",
    "        raise\n",
    "\n",
    "def load_results(context: MLClientCtx,\n",
    "                 dask_client: str,\n",
    "                 connection_info: str,\n",
    "                 query: str,\n",
    "                 parquet_out_dir = None,\n",
    "                 publish_name = None\n",
    "                ) -> None:\n",
    "\n",
    "    \"\"\"Snowflake Dask - Ingest Snowflake data with Dask\n",
    "\n",
    "    :param context:           the function context\n",
    "    :param dask_client:       dask cluster function name\n",
    "    :param connection_info:   Snowflake database connection info (this will be in a secret later)\n",
    "    :param query:             query to for Snowflake\n",
    "    :param parquet_out_dir:   directory path for the output parquet files\n",
    "                              (default None, not write out)\n",
    "    :param publish_name:      name of the dask dataframe to publish to the dask cluster\n",
    "                              (default None, not publish)\n",
    "\n",
    "    \"\"\"\n",
    "    context = mlrun.get_or_create_ctx('snawflake-dask-cluster')\n",
    "    sf_password = context.get_secret('sfPassword')\n",
    "    pk_path =  context.get_secret('pkPath')\n",
    "    pk_password =  context.get_secret('pkPassword')\n",
    "\n",
    "    if pk_path and pk_password:\n",
    "        with open(pk_path, \"rb\") as key:\n",
    "            p_key= serialization.load_pem_private_key(\n",
    "                key.read(),\n",
    "                password=str(pk_password).encode(),\n",
    "                backend=default_backend()\n",
    "            )\n",
    "        pkb = p_key.private_bytes(\n",
    "            encoding=serialization.Encoding.DER,\n",
    "            format=serialization.PrivateFormat.PKCS8\n",
    "            ,encryption_algorithm=serialization.NoEncryption()\n",
    "        )\n",
    "        connection_info.pop('password', 'No password found')\n",
    "        connection_info['private_key'] = pkb\n",
    "    elif sf_password:\n",
    "        connection_info['password'] = sf_password\n",
    "    else:\n",
    "        raise Exception(\"\\nPlease set up the secret for Snowflake in your project!\\n\")\n",
    "\n",
    "    # setup dask client from the MLRun dask cluster function\n",
    "    if dask_client:\n",
    "        client = mlrun.import_function(dask_client).client\n",
    "        context.logger.info(f'Existing dask client === >>> {client}\\n')\n",
    "    else:\n",
    "        client = Client()\n",
    "        context.logger.info(f'\\nNewly created dask client === >>> {client}\\n')\n",
    "\n",
    "    conn = snow.connect(**connection_info)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query)\n",
    "    batches = cur.get_result_batches()\n",
    "    context.logger.info(f'batches len === {len(batches)}\\n')\n",
    "\n",
    "    dfs = []\n",
    "    for batch in batches:\n",
    "        if batch.rowcount > 0:\n",
    "            df = load(batch)\n",
    "            dfs.append(df)\n",
    "    ddf = from_delayed(dfs)\n",
    "\n",
    "    # materialize the query results set for some sample compute\n",
    "\n",
    "    ddf_describe = ddf.describe().compute()\n",
    "\n",
    "    context.logger.info(f'query  === >>> {query}\\n')\n",
    "    context.logger.info(f'ddf  === >>> {ddf}\\n')\n",
    "    context.log_result('number of rows', len(ddf.index))\n",
    "    context.log_dataset(\"ddf_describe\", df=ddf_describe)\n",
    "\n",
    "    if publish_name:\n",
    "        context.log_result('data_set_name', publish_name)\n",
    "        if not client.list_datasets():\n",
    "            ddf.persist(name = publish_name)\n",
    "            client.publish_dataset(publish_name=ddf)\n",
    "\n",
    "    if parquet_out_dir:\n",
    "        dd.to_parquet(df=ddf, path=parquet_out_dir)\n",
    "        context.log_result('parquet directory', parquet_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the code to MLRun function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code_to_function to convert the code to MLRun <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = mlrun.code_to_function(name=\"snowflake-dask\",  \n",
    "                            kind='job', \n",
    "                            filename='snowflake_dask.py',\n",
    "                            image='mlrun/mlrun',\n",
    "                            requirements='requirements.txt',\n",
    "                            handler=\"load_results\", \n",
    "                            description=\"Snowflake Dask - Ingest snowflake data in parallel with Dask cluster\",\n",
    "                            categories=[\"data-prep\"],\n",
    "                            labels={\"author\": \"xingsheng\"}\n",
    "                           )\n",
    "fn.apply(mlrun.platforms.auto_mount())\n",
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export function to local `function.yaml` file for testing\n",
    "in the real usage, we will import a function from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.export('function.yaml')\n",
    "# print(fn.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import a function from local `function.yaml' for testing (Need to change it to import from hub before PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = mlrun.import_function(\"./function.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = mlrun.import_function(\"hub://snowflake_dask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.apply(mlrun.platforms.auto_mount()) # this is a very important line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a dask cluster and specify the configuration for the dask process (e.g. replicas, memory etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function URI is db://<project>/<name>\n",
    "dask_uri = f'db://{project_name}/{dask_cluster_name}'\n",
    "dask_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf = mlrun.new_function(name=dask_cluster_name, \n",
    "                         kind='dask', \n",
    "                         image='mlrun/mlrun',\n",
    "                         requirements=[\"bokeh\", \"snowflake-connector-python[pandas]\"]\n",
    "                        )\n",
    "dsf.apply(mlrun.mount_v3io())\n",
    "dsf.spec.remote = True\n",
    "dsf.spec.min_replicas = 1\n",
    "dsf.spec.max_replicas = 10\n",
    "dsf.spec.service_type = \"NodePort\"\n",
    "dsf.with_requests(mem='4G', cpu='2')\n",
    "# dsf.spec.node_port=30088\n",
    "# dsf.spec.scheduler_timeout = \"5 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dsf.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the function you would see a remote dashboard link as part of the result. click on this link takes you to the dask monitoring dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'my-local-test'\n",
    "parquet_path = f\"/v3io/bigdata/pq_from_sf_dask/{p}\"\n",
    "\n",
    "fn.run(handler = 'load_results',\n",
    "       params={\"dask_client\": dask_uri, \n",
    "               \"connection_info\": connection_info, \n",
    "               \"query\": \"SELECT * FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\",\n",
    "               \"parquet_out_dir\": parquet_path,\n",
    "               \"publish_name\": \"customer\",\n",
    "              }\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the progress in the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can view the progress and detailed information in the mlrun UI by clicking on the uid above. <br>\n",
    "Also, to track the dask progress in the dask UI click on the \"dashboard link\" above the \"client\" section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
